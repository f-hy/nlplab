{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-0.5.0", "display_name": "MindSpore-0.5.0-python3.7-aarch64", "language": "python"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "## MindSpore-BERT-NER\n### 1. \u4e0b\u8f7d\u6e90\u7801\u548c\u6570\u636e\u81f3\u672c\u5730\u5bb9\u5668\n\n\u56e0\u4e3anotebook\u662f\u6302\u8f7d\u5728obs\u4e0a\uff0c\u8fd0\u884c\u7684\u5bb9\u5668\u5b9e\u4f8b\u4e0d\u80fd\u76f4\u63a5\u8bfb\u53d6\u64cd\u4f5cobs\u4e0a\u7684\u6587\u4ef6\uff0c\u9700\u4e0b\u8f7d\u81f3\u5bb9\u5668\u672c\u5730\u73af\u5883\u4e2d", "metadata": {}}, {"cell_type": "code", "source": "import moxing as mox\nmox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/bert_ner_notebook/src/\", dst_url='./src/') \nmox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/bert_ner_notebook/data/\", dst_url='./data/')\nmox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/bert_ner_notebook/pre_model/\", dst_url='./pre_model/')", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v1.17.3-43fbf97f\nINFO:root:Using OBS-Python-SDK-3.20.7\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### 2. \u5bfc\u5165\u4f9d\u8d56\u5e93", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport argparse\nimport numpy as np\nfrom easydict import EasyDict as edict\n\nimport mindspore.nn as nn\nimport mindspore.common.dtype as mstype\nfrom mindspore.common.initializer import TruncatedNormal\nfrom mindspore import context\nfrom mindspore import log as logger\nfrom mindspore.common.tensor import Tensor\nimport mindspore.dataset as de\nfrom mindspore.ops import operations as P\nimport mindspore.dataset.transforms.c_transforms as C\nfrom mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell\nfrom mindspore.nn.optim import AdamWeightDecayDynamicLR, Lamb, Momentum\nfrom mindspore.train.model import Model\nfrom mindspore.train.callback import Callback\nfrom mindspore.train.callback import CheckpointConfig, ModelCheckpoint, LossMonitor\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\n\nfrom src.CRF import CRF\nfrom src.utils import Accuracy, F1, BertFinetuneCell\nfrom src.config import tag_to_index, bert_optimizer_cfg\nfrom src.bert_model import BertConfig, BertModel\nfrom src.cluener_evaluation import ner_process", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "### 3. \u5b9a\u4e49\u53c2\u6570\u914d\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "cfg = edict({\n    'task': 'NER',                    \n    'num_labels': 41,                 \n    'schema_file': r'./data/clue_ner/schema.json',      \n    'ckpt_prefix': 'bert-ner-crf',          \n    \n    'train_file': r'./data/clue_ner/train.tf_record',\n    'eval_file': r'./data/clue_ner/dev.tf_record',\n    'batch_size': 16,\n    'epoch_num': 5,\n    'ckpt_dir': 'ckpt',\n   \n    'pre_training_ckpt': './pre_model/bert_base.ckpt',\n    'finetune_ckpt': './ckpt/bert-ner-crf-5_671.ckpt',   \n    \n    'label2id_file': './data/clue_ner/label2id.json',       \n    'vocab_file': './data/vocab.txt',\n    'use_crf': True,\n})\n\nbert_net_cfg = BertConfig(\n    batch_size=cfg.batch_size,\n    seq_length=128,\n    vocab_size=21128,\n    hidden_size=768,\n    num_hidden_layers=12,\n    num_attention_heads=12,\n    intermediate_size=3072,\n    hidden_act=\"gelu\",\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n    max_position_embeddings=512,\n    type_vocab_size=2,\n    initializer_range=0.02,\n    use_relative_positions=False,\n    input_mask_from_dataset=True,\n    token_type_ids_from_dataset=True,\n    dtype=mstype.float32,\n    compute_type=mstype.float16,  \n)", "metadata": {"trusted": true}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "### 4. \u5b9a\u4e49\u6570\u636e\u96c6\u52a0\u8f7d\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def get_dataset(data_file, batch_size=1, repeat_count=1):\n    '''\n    get dataset\n    '''\n    ds = de.TFRecordDataset([data_file], cfg.schema_file, columns_list=[\"input_ids\", \"input_mask\",\"segment_ids\", \"label_ids\"])\n    type_cast_op = C.TypeCast(mstype.int32)\n    ds = ds.map(input_columns=\"segment_ids\", operations=type_cast_op)\n    ds = ds.map(input_columns=\"input_mask\", operations=type_cast_op)\n    ds = ds.map(input_columns=\"input_ids\", operations=type_cast_op)\n    ds = ds.map(input_columns=\"label_ids\", operations=type_cast_op)\n    \n    # apply shuffle operation\n    ds = ds.shuffle(buffer_size=900)\n\n    # apply batch operations\n    ds = ds.batch(batch_size, drop_remainder=True)\n    ds = ds.repeat(repeat_count)\n    return ds", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": "\u6570\u636e\u96c6\u6d4b\u8bd5", "metadata": {}}, {"cell_type": "code", "source": "get_dataset(cfg.train_file).create_dict_iterator().get_next()['input_ids'][0]", "metadata": {"trusted": true}, "execution_count": 6, "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "array([ 101, 7213, 6121,  679, 2533, 1403, 3313, 4007,  122,  129, 1453,\n       2259, 4638, 2110, 4495, 1355, 1305, 8024, 7557, 5862, 2141, 5018,\n        753, 6820, 3621, 3341, 3975, 5023,  511,  127,  121, 1384, 3152,\n       6206, 3724, 8024,  102,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0], dtype=int32)"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "### 5. \u5b9a\u4e49BertNER\u6a21\u578b", "metadata": {}}, {"cell_type": "code", "source": "class BertNER(nn.Cell):\n    \"\"\"\n    Train interface for sequence labeling finetuning task.\n    \"\"\"\n    def __init__(self, config, is_training, num_labels=11, use_crf=False, tag_to_index=None, dropout_prob=0.0,\n                 use_one_hot_embeddings=False):\n        super(BertNER, self).__init__()\n        self.bert = BertModel(config, is_training, use_one_hot_embeddings)\n        self.cast = P.Cast()\n        self.weight_init = TruncatedNormal(config.initializer_range)\n        self.log_softmax = P.LogSoftmax(axis=-1)\n        self.dtype = config.dtype\n        self.num_labels = num_labels\n        self.dense_1 = nn.Dense(config.hidden_size, self.num_labels, weight_init=self.weight_init,\n                                has_bias=True).to_float(config.compute_type)\n        self.dropout = nn.Dropout(1 - dropout_prob)\n        self.reshape = P.Reshape()\n        self.shape = (-1, config.hidden_size)\n        self.use_crf = use_crf\n        self.origin_shape = (config.batch_size, config.seq_length, self.num_labels)\n        if use_crf:\n            if not tag_to_index:\n                raise Exception(\"The dict for tag-index mapping should be provided for CRF.\")\n            self.loss = CRF(tag_to_index, config.batch_size, config.seq_length, is_training)\n        else:\n            self.loss = CrossEntropyCalculation(is_training)\n        self.num_labels = num_labels\n        self.use_crf = use_crf\n        \n    def construct(self, input_ids, input_mask, token_type_id, label_ids):\n        sequence_output, _, _ = \\\n            self.bert(input_ids, token_type_id, input_mask)\n        seq = self.dropout(sequence_output)\n        seq = self.reshape(seq, self.shape)\n        logits = self.dense_1(seq)\n        logits = self.cast(logits, self.dtype)\n        \n        if self.use_crf:\n            return_value = self.reshape(logits, self.origin_shape)\n            loss = self.loss(return_value, label_ids)\n        else:\n            return_value = self.log_softmax(logits)\n            loss = self.loss(return_value, label_ids, self.num_labels)\n        return loss", "metadata": {"trusted": true}, "execution_count": 7, "outputs": []}, {"cell_type": "markdown", "source": "### 6. \u5b9a\u4e49\u8bad\u7ec3\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def train():\n    devid = int(os.getenv('DEVICE_ID'))\n    context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\", device_id=devid)\n    \n    if cfg.use_crf:\n        netwithloss = BertNER(bert_net_cfg, True, num_labels=len(tag_to_index), use_crf=True,\n                              tag_to_index=tag_to_index, dropout_prob=0.1)\n    else:\n        netwithloss = BertNER(bert_net_cfg, True, num_labels=cfg.num_labels, dropout_prob=0.1)\n        \n    dataset = get_dataset(cfg.train_file, bert_net_cfg.batch_size, repeat_count=cfg.epoch_num)\n    \n    # optimizer\n    steps_per_epoch = dataset.get_dataset_size()\n    optimizer = AdamWeightDecayDynamicLR(netwithloss.trainable_params(),\n                                         decay_steps=steps_per_epoch * cfg.epoch_num,\n                                         learning_rate=bert_optimizer_cfg.AdamWeightDecayDynamicLR.learning_rate,\n                                         end_learning_rate=bert_optimizer_cfg.AdamWeightDecayDynamicLR.end_learning_rate,\n                                         power=bert_optimizer_cfg.AdamWeightDecayDynamicLR.power,\n                                         warmup_steps=int(steps_per_epoch * cfg.epoch_num * 0.1),\n                                         weight_decay=bert_optimizer_cfg.AdamWeightDecayDynamicLR.weight_decay,\n                                         eps=bert_optimizer_cfg.AdamWeightDecayDynamicLR.eps)\n    \n    # load checkpoint into network\n    ckpt_config = CheckpointConfig(save_checkpoint_steps=steps_per_epoch, keep_checkpoint_max=1)\n    ckpoint_cb = ModelCheckpoint(prefix=cfg.ckpt_prefix, directory=cfg.ckpt_dir, config=ckpt_config)\n    param_dict = load_checkpoint(cfg.pre_training_ckpt)\n    load_param_into_net(netwithloss, param_dict)\n\n    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=2**32, scale_factor=2, scale_window=1000)\n    netwithgrads = BertFinetuneCell(netwithloss, optimizer=optimizer, scale_update_cell=update_cell)\n    model = Model(netwithgrads)\n    model.train(cfg.epoch_num, dataset, callbacks=[LossMonitor(), ckpoint_cb], dataset_sink_mode=True)", "metadata": {"trusted": true}, "execution_count": 8, "outputs": []}, {"cell_type": "markdown", "source": "### 7. \u542f\u52a8\u8bad\u7ec3", "metadata": {}}, {"cell_type": "code", "source": "train()", "metadata": {"trusted": true}, "execution_count": 9, "outputs": [{"name": "stderr", "text": "[WARNING] ME(119:281473092671552,MainProcess):2020-11-04-12:36:37.467.296 [mindspore/train/serialization.py:291] Remove parameter prefix name: bert., continue to load.\n", "output_type": "stream"}, {"name": "stdout", "text": "epoch: 1 step 671, loss is 15.178092956542969\nEpoch time: 369228.570, per step time: 550.266, avg loss: 15.178\n************************************************************\nepoch: 2 step 671, loss is 6.42706298828125\nEpoch time: 40811.917, per step time: 60.823, avg loss: 6.427\n************************************************************\nepoch: 3 step 671, loss is 4.193878173828125\nEpoch time: 40576.970, per step time: 60.472, avg loss: 4.194\n************************************************************\nepoch: 4 step 671, loss is 4.1775360107421875\nEpoch time: 40547.132, per step time: 60.428, avg loss: 4.178\n************************************************************\nepoch: 5 step 671, loss is 1.4002304077148438\nEpoch time: 40637.079, per step time: 60.562, avg loss: 1.400\n************************************************************\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### 8. \u5b9a\u4e49\u6d4b\u8bd5\u96c6\u8bc4\u4f30\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def evaluate():\n    context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")\n    \n    dataset = get_dataset(cfg.eval_file, bert_net_cfg.batch_size, repeat_count=1)\n    net_for_pretraining = BertNER(bert_net_cfg, False, num_labels=len(tag_to_index), use_crf=cfg.use_crf,\n                                         tag_to_index=tag_to_index, dropout_prob=0.0)\n    net_for_pretraining.set_train(False)\n    param_dict = load_checkpoint(cfg.finetune_ckpt)\n    load_param_into_net(net_for_pretraining, param_dict)\n    model = Model(net_for_pretraining)\n    \n    callback = F1()\n    columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_ids\"]\n    for data in dataset.create_dict_iterator():\n        input_data = []\n        for i in columns_list:\n            input_data.append(Tensor(data[i]))\n        input_ids, input_mask, token_type_id, label_ids = input_data\n        logits = model.predict(input_ids, input_mask, token_type_id, label_ids)\n        callback.update(logits, label_ids)\n    print(\"==============================================================\")\n    print(\"Precision {:.6f} \".format(callback.TP / (callback.TP + callback.FP)))\n    print(\"Recall {:.6f} \".format(callback.TP / (callback.TP + callback.FN)))\n    print(\"F1 {:.6f} \".format(2*callback.TP / (2*callback.TP + callback.FP + callback.FN)))\n    print(\"==============================================================\")", "metadata": {"trusted": true}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": "### 9. \u542f\u52a8\u6d4b\u8bd5\u96c6\u8bc4\u4f30", "metadata": {}}, {"cell_type": "code", "source": "evaluate()", "metadata": {"trusted": true}, "execution_count": 11, "outputs": [{"name": "stdout", "text": "==============================================================\nPrecision 0.919682 \nRecall 0.954276 \nF1 0.936659 \n==============================================================\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### 10. \u5b9a\u4e49\u5728\u7ebf\u63a8\u7406\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def inference(text):\n    context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")\n    bert_net_cfg.batch_size = 1\n    net_for_pretraining = BertNER(bert_net_cfg, False, num_labels=len(tag_to_index), use_crf=cfg.use_crf,\n                                         tag_to_index=tag_to_index, dropout_prob=0.0)\n    net_for_pretraining.set_train(False)\n    param_dict = load_checkpoint(cfg.finetune_ckpt)\n    load_param_into_net(net_for_pretraining, param_dict)\n    model = Model(net_for_pretraining)\n    res = ner_process(model, text, bert_net_cfg.seq_length, cfg)\n    print(\"text\", text)\n    print(\"res:\", res)\n    \n    return res", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": "### 11. \u5728\u7ebf\u63a8\u7406\u6d4b\u8bd5", "metadata": {}}, {"cell_type": "code", "source": "inference(\"\u5f6d\u5c0f\u519b\u8ba4\u4e3a\uff0c\u56fd\u5185\u94f6\u884c\u73b0\u5728\u8d70\u7684\u662f\u53f0\u6e7e\u7684\u53d1\u5361\u6a21\u5f0f\uff0c\u5148\u901a\u8fc7\u8dd1\u9a6c\u5708\u5730\u518d\u5728\u5708\u7684\u5730\u91cc\u9762\u9009\u62e9\u5ba2\u6237\")", "metadata": {}, "execution_count": 11, "outputs": [{"output_type": "stream", "text": "text \u5f6d\u5c0f\u519b\u8ba4\u4e3a\uff0c\u56fd\u5185\u94f6\u884c\u73b0\u5728\u8d70\u7684\u662f\u53f0\u6e7e\u7684\u53d1\u5361\u6a21\u5f0f\uff0c\u5148\u901a\u8fc7\u8dd1\u9a6c\u5708\u5730\u518d\u5728\u5708\u7684\u5730\u91cc\u9762\u9009\u62e9\u5ba2\u6237\nres: {'name': {'\u5f6d\u5c0f\u519b': [[0, 2]]}, 'address': {'\u53f0\u6e7e': [[15, 16]]}}\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "{'name': {'\u5f6d\u5c0f\u519b': [[0, 2]]}, 'address': {'\u53f0\u6e7e': [[15, 16]]}}"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "### 12. \u5c06\u6a21\u578bcheckpoint\u6587\u4ef6\u56de\u4f20obs", "metadata": {}}, {"cell_type": "code", "source": "mox.file.copy_parallel(src_url=\"./ckpt/\", dst_url=\"s3://ascend-zyjs-dcyang/nlp/bert_ner_notebook/ckpt/\")", "metadata": {"trusted": true}, "execution_count": 12, "outputs": []}]}